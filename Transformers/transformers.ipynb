{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e98943",
   "metadata": {},
   "source": [
    "# Transformers on SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d955673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c704e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "smiles = data[\"smiles\"]\n",
    "y = data[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d098705",
   "metadata": {},
   "source": [
    "### Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d68aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split 80% train and 20% test\n",
    "# train_smiles, test_smiles, train_labels, test_labels = train_test_split(smiles, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train80 = pd.read_csv(\"train_data_80.csv\")\n",
    "train2O = pd.read_csv(\"train_data_20.csv\")\n",
    "\n",
    "train_smiles = train80[\"smiles\"]\n",
    "train_labels = train80[\"class\"]\n",
    "test_smiles = train2O[\"smiles\"]\n",
    "test_labels = train2O[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385c908",
   "metadata": {},
   "source": [
    "### Convert to a dictionary format for Hugging Face's Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = {\"smiles\": train_smiles.tolist(), \"labels\": train_labels.tolist()}\n",
    "# test_data = {\"smiles\": test_smiles.tolist(), \"labels\": test_labels.tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b8cc89",
   "metadata": {},
   "source": [
    "### Fct° compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a280ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    kappa = cohen_kappa_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"cohen_kappa\": kappa,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d3463",
   "metadata": {},
   "source": [
    "### SMILESDateset Class and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_list, labels, tokenizer):\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.smiles_list = smiles_list.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        label = self.labels[idx]  \n",
    "        inputs = self.tokenizer(smiles, max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}  # Remove batch dimension\n",
    "        inputs[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd06b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SMILESDataset(train_smiles, train_labels, tokenizer)\n",
    "test_dataset = SMILESDataset(test_smiles, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c1a78",
   "metadata": {},
   "source": [
    "### Load pre-trained ChemBERTa and configure it for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf2cd9",
   "metadata": {},
   "source": [
    "### Set up the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.005,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    #tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328fc06a",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97edb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de2ea0",
   "metadata": {},
   "source": [
    "## Evaluate and calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016a0aa",
   "metadata": {},
   "source": [
    "### Calculate metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "print(classification_report(test_labels, predicted_labels))\n",
    "print(\"Cohen's Kappa Score:\", cohen_kappa_score(test_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025301e",
   "metadata": {},
   "source": [
    "### Predictions on train.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07015ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "proba_toxicite = predictions.predictions\n",
    "if proba_toxicite.shape[1] == 2:\n",
    "    # Softmax pour obtenir les probabilités\n",
    "    proba_toxicite = torch.softmax(torch.tensor(proba_toxicite), dim=1).numpy()[:, 1]\n",
    "else:\n",
    "    # Si déjà des probabilités, prendre la colonne 1\n",
    "    proba_toxicite = proba_toxicite[:, 1]\n",
    "\n",
    "df_proba = pd.DataFrame({\n",
    "    \"smiles\": test_smiles.values,\n",
    "    \"proba_toxicite\": proba_toxicite\n",
    "})\n",
    "\n",
    "df_proba.to_csv(\"smiles_proba.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff96393",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(df_proba[\"smiles\"].values == test_smiles.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd1bf3",
   "metadata": {},
   "source": [
    "### Prediction on test_1.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e68e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prédictions sauvegardées dans test_1_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, cohen_kappa_score\n",
    "\n",
    "# === 1. Définir le dataset pour les SMILES ===\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_list, labels, tokenizer):\n",
    "        self.smiles_list = smiles_list.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(smiles, max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        inputs[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "# === 2. Charger le tokenizer et le modèle entraîné ===\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./results/checkpoint-590\")\n",
    "\n",
    "\n",
    "# === 3. Préparer le fichier test_1.csv ===\n",
    "new_data = pd.read_csv(\"data/test_1.csv\")\n",
    "new_smiles = new_data[\"smiles\"]\n",
    "\n",
    "# Créer un dataset avec des labels factices (nécessaires pour l'objet Dataset)\n",
    "dummy_labels = pd.Series([0] * len(new_smiles))\n",
    "new_dataset = SMILESDataset(new_smiles, dummy_labels, tokenizer)\n",
    "\n",
    "# === 4. Configurer le Trainer pour l’inférence uniquement ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Chemin arbitraire\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_dir=\"./logs\",\n",
    "    do_train=False,  # Pas d'entraînement\n",
    "    do_eval=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# === 5. Prédictions ===\n",
    "predictions = trainer.predict(new_dataset)\n",
    "logits = predictions.predictions\n",
    "\n",
    "# Convertir les logits en probabilités\n",
    "if logits.shape[1] == 2:\n",
    "    proba_toxicite = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "else:\n",
    "    proba_toxicite = logits[:, 1]\n",
    "\n",
    "# === 6. Exporter les résultats ===\n",
    "df_resultats = pd.DataFrame({\n",
    "    \"smiles\": new_smiles,\n",
    "    \"proba_toxicite\": proba_toxicite\n",
    "})\n",
    "\n",
    "df_resultats.to_csv(\"test_1_predictions.csv\", index=False)\n",
    "print(\"✅ Prédictions sauvegardées dans test_1_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
